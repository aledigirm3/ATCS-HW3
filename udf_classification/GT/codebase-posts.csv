"id","body"
"60822","<p>The $z$-value is just the test-statistic for a statistical test, so if you have trouble interpreting it your first step is to find out what the null hypothesis is. The null hypothesis for the test for CLASS0 is that its coefficient is 0. The coefficient for CLASS0 is the difference in log(odds) between CLASS0 and the reference class (CLASS3?) is zero, or equivalently, that the ratio of the odds for CLASS0 and the reference class is 1. In other words that there is no difference in the odds of success between CLASS0 and the reference class.</p>

<p>So does a non-significant coefficient mean you can merge categories? No. First, non-significant means that we cannot reject the hypothesis that there is no difference, but that does not mean that no such differences exist. An absence of evidence is not the same thing as evidence of absence. Second, merging categories, especially the reference category, changes the interpretation of all other coefficients. Whether or not that makes sense depends on what those different classes stand for. </p>

<p>Does that mean that the entire categorical variable is a ""bad"" (non-significant) predictor? No, for that you would need to perform a simultaneous test for all CLASS terms. </p>
"
"22227","<p>Prior $P(buys)$ is constant for each person. Some people buy more, others buy less, but for any particular person, if you don't know anything about the product he's looking at, the probability of buying is some constant.</p>

<p>This means that, to find the posterior $P(buys|X)$, you don't actually have to know $P(buys)$. </p>

<p>$P(buys|X)$ = $P(X|buys) P(buys) / P(X) = k P(X|buys)$, where $k$ is a constant. To maximize $P(buys|X)$, you thus just maximize $P(X|buys)$.</p>

<p>The much bigger problem is that for any particular individual, you will have too little data to go on. A solution is to somehow group people who you think are similar. You could use their demographics (age, sex, location), but people on your site from the same demographic might be shopping for different things. In addition to demographics, you could also use the product category that they've bought in the past. So, for example, group all 25-35 year old women on the West Coast who've bought a computer-related product in the past into a single ""person"".</p>
"
"111387","<p>The short answer is that you're trying to model a regression with more slope parameters then you have observations.</p>

<p>For more meaningful results, try limiting the number of interaction terms you try to model. I mean, do you really want to have terms for <em>every conceivable nonempty subset of your predictors</em>? (For instance, do you really think that the <code>gearsc:length:depth:in_water:condition4</code> coefficient is the key to your analysis?). </p>

<p>The result is that you'll have to do considerably more typing (since you'll be specifying the individual interaction terms instead of merely multiplying all the coefficients together in the formula). However, the upside is that you'll end up with stronger results.</p>

<p>Does this help?</p>

<h3>Edit:</h3>

<p>Now, if you really <em>do</em> want to model all those interaction terms, there are certain approaches you <em>could</em> take. For instance, you can check out this set of <a href=""https://www.cs.cmu.edu/~ggordon/10725-F12/scribes/10725_Lecture2.pdf"" rel=""nofollow"">lecture notes</a> for strategies for dealing with <strong>sparse matrices</strong>. However, I think the easiest thing here would be to manually add a handful of interaction terms since it's usually best to keep things as simple as they need to be (but no simpler).</p>
"
"102898","<p>Is this what you have in mind?</p>

<pre><code>sysuse auto
lowess price mpg, gen(yhat)
tw (line yhat mpg if price &lt;12000, sort) (scatter yhat mpg if price &lt;12000)
</code></pre>

<p>In practice, this seems like a curious thing to do. </p>
"
"4235","<p>Yes.  I like the article Soren shared very much, and together with the references in that article I would recommend Muckenheim, W. <em>et al.</em> (1986). <a href=""http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6TVP-46SX0JS-88&amp;_user=10&amp;_coverDate=02%2F28%2F1986&amp;_rdoc=1&amp;_fmt=high&amp;_orig=search&amp;_origin=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_searchStrId=1530555242&amp;_rerunOrigin=google&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=1fc99b45c91f0d7e00c238a3649d7532&amp;searchtype=a"" rel=""nofollow"">A Review of Extended Probabilities</a>. Phys. Rep. 133 (6) 337-401.  It's a physics paper for sure, but the applications there are not all related to quantum physics.</p>

<p>My personal favorite application relates to <a href=""http://en.wikipedia.org/wiki/De_Finetti%27s_theorem"" rel=""nofollow"">de Finetti's Theorem</a> (also Bayesian in flavor):  if we don't mind negative probabilities then it turns out that <strong>all</strong> exchangeable sequences (even finite, perhaps negatively correlated ones) are a (signed) mixture of IID sequences.  Of course, this itself has applications in quantum mechanics, in particular, that Fermi-Dirac statistics yield the same type of (signed) mixture representation that Bose-Einstein statistics do. </p>

<p>My second personal favorite application (outside of physics proper) relates to <a href=""http://en.wikipedia.org/wiki/Infinite_divisibility_%28probability%29"" rel=""nofollow"">infinite divisible</a> (ID) distributions, which classically includes normal, gamma, poisson, ... the list continues.  It isn't too hard to show that ID distributions must have unbounded support, which immediately kills distributions like the binomial or uniform (discrete+continuous) distributions.  But if we permit negative probabilities then these problems disappear and the binomial, uniform (discrete+continuous), and a whole bunch of other distributions then become infinitely divisible - in this <em>extended</em> sense, please bear in mind.  ID distributions relate to statistics in that they are limiting distributions in generalized central limit theorems.</p>

<p>By the way, the first application is whispered folklore among probabilists and the infinite divisibility stuff is proved <a href=""http://maurice.bgsu.edu/record=b2719985~S0"" rel=""nofollow"">here</a>, an informal electronic copy being <a href=""http://people.ysu.edu/~gkerns/pdf/Jdiss.pdf"" rel=""nofollow"">here</a>.</p>

<p>Presumably there is a bunch of material on <a href=""http://www.arxiv.org/"" rel=""nofollow"">arXiv</a>, too, though I haven't checked there in quite some time.</p>

<p>As a final remark, whuber is absolutely right that it isn't really legal to call anything a probability that doesn't lie in $[0,1]$, at the very least, not for the time being. Given that ""negative probabilities"" have been around for so long I don't see this changing in the near future, not without some kind of colossal breakthrough.</p>
"
"25434","<p>You may want to consider other strategies based on propensity scores, like including them as model covariates, or very similar concepts, like Inverse-Probability-of-Treatment weights. These might work in situations where you can't, or don't want, to deal with matching.</p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1407370/"" rel=""nofollow"">This</a> seems like a decent overview.</p>
"
"92683","<p>Bookwise: 
The uber-classic is <a href=""http://cm.bell-labs.com/cm/ms/departments/sia/NLME/MEMSS/index.html"" rel=""nofollow"">Mixed-Effects Models in S and S-PLUS</a> by Bates and Pinheiro; it shows its years at time but it is still an excellent read. I have also read <a href=""http://www.crcpress.com/product/isbn/9781439815120"" rel=""nofollow"">Generalized Linear Mixed Models: Modern Concepts, Methods and Applications</a> by Stroup; it is quite new and based on software I have never used (SAS) but it very well-written and clear. I like it a lot. I believe those two resources will cover most questions. I have also found <a href=""http://www.crcpress.com/product/isbn/9781584884804"" rel=""nofollow"">Linear Mixed Models: A Practical Guide Using Statistical Software</a> by West,  Welch and Galecki to be a good read but I would not pick it over the previous two. Finally <a href=""http://www.highstat.com/book2.htm"" rel=""nofollow"">Mixed Effects Models and Extensions in Ecology with R</a> by Zuur, Ieno, Walker, Saveliev and Smith seems to be popular but I have not used personally.</p>

<p>The Centre for Multilevel Modelling (CMM) in the University of Bristol has a fuller web-catalogue of good books that can be found <a href=""http://www.bristol.ac.uk/cmm/learning/support/books.html"" rel=""nofollow"">here</a>.</p>

<p>Paper-wise:
This list can be endless and probably you can find works specific to your field of application. Having said that the paper: <a href=""http://facecouncil.org/puf/wp-content/uploads/2009/10/Bolker-et-al-2009-TREE.pdf"" rel=""nofollow"">Generalized linear mixed models: a practical guide for ecology and evolution</a> by Bolker, Brooks, Clark, Geange, Poulsen, Stevens and White is pretty darn good (I like <a href=""http://www.cell.com/trends/ecology-evolution/home"" rel=""nofollow"">TREE</a> papers).</p>

<p>And a side-comment:
Given <a href=""http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/lme4/html/lmer.html"" rel=""nofollow"">lmer</a>/<a href=""http://stat.ethz.ch/R-manual/R-patched/library/nlme/html/lme.html"" rel=""nofollow"">lme</a>/<a href=""http://www.mathworks.co.uk/help/stats/fitlme.html"" rel=""nofollow"">fitlme</a>/<a href=""http://www.vsni.co.uk/software/genstat/htmlhelp/reml/LinearMixedModelsREML.htm"" rel=""nofollow"">genstat</a> are coherent pieces of software, what you want is to understand how <a href=""http://www.jstor.org/discover/10.2307/2346786"" rel=""nofollow"">Wilkinson notation</a> works. There is not ""magic"" surrounding how you define interactions and nesting, just some rules that are (generally) not software-specific.</p>
"
"41337","<p>Multinomial logistic is the right method, I think. But it seems like you might need a multilevel model because you have multiple items for each person and those choices won't be independent. </p>

<p>This is equivalent to running many dichotomous logistic regressions, one on each type of item. </p>
"
"4633","<p>As @onestop writes, the GM and GSD are natural parameters for a lognormal distribution.  However, they can be estimated from the arithmetic mean ($\\mu$) and (usual) SD ($\\sigma$) just by solving the <a href=""http://en.wikipedia.org/wiki/Log-normal_distribution"" rel=""nofollow"">formulas</a></p>

<p>$$\\mu = \\exp(\\nu+ \\tau^2/2) \\text{ and}$$
</p>

<p>$$\\sigma^2 = \\left( \\exp(\\tau^2)-1\\right) \\exp(2 \\nu+ \\tau^2) = \\left( \\exp(\\tau^2)-1\\right)  \\mu^2$$
</p>

<p>for $\\nu$ (the logarithm of the GM) and $\\tau$ (the logarithm of the GSD).  Evidently</p>

<p>$$\\tau^2 = \\log{\\frac{\\sigma^2 + \\mu^2}{\\mu^2}} \\text{ and}$$
</p>

<p>$$\\nu = \\frac{1}{2} \\log{\\frac{\\mu^4}{\\sigma^2 + \\mu^2}}.$$
</p>

<p>The distribution of the <em>logarithms</em> of the particle sizes is Normal with mean $\\nu$ and variance $\\tau^2$, reducing your problem to the elementary one of computing (or looking up) values of the cumulative normal distribution.</p>
"
"77809","<p>You can change the loss function that is used for fitting the model parameters and evaluating the forecasts. For binary models, there is a large class of loss functions called ""proper scoring rules"", including e.g. negative likelihood and squared error. These loss functions are sensible from a stat theory perspective since they set the incentive to identify the true model asymptotically. The class contains asymmetric members which you could use in your application. For a good review, see </p>

<p><a href=""http://stat.wharton.upenn.edu/~buja/PAPERS/paper-proper-scoring.pdf"" rel=""nofollow"">http://stat.wharton.upenn.edu/~buja/PAPERS/paper-proper-scoring.pdf</a></p>
"
"13489","<p>I suspect it means very little in your actual figure; you have drawn a form of stripplot/chart. But as we don't have the data or reproducible example, I will just describe what these lines/regions show in general.</p>

<p>In general, the line is the fitted linear model describing the relationship $$\\widehat{\\mathrm{val}} = \\beta_0 + \\beta_1 \\mathrm{Num}$$ The shaded band is a pointwise 95% confidence interval on the fitted values (the line). This confidence interval contains the true, population, regression line with 0.95 probability. Or, in other words, there is 95% confidence that the true regression line lies within the shaded region. It shows us the uncertainty inherent in our estimate of the true relationship between your response and the predictor variable.</p>
"
"30081","<p>It really sounds like you want to do cross-validation.  I am assuming you use the data to construct the criterion.  A way to do this that is a type of cross validation would be to randomly split the data into tow groups ( could be an equal split but you may want a larger sample for the fitting of the criterion).  Construct the criterion on the first set and then test how it does.</p>
"
"47384","<blockquote>
  <p>What is this, I don't even... Jimmy Rustles.</p>
</blockquote>

<p>One of my favourites. </p>
"
"30567","<p>As Michael notes, when comparing a subgroup to an overall group, researchers typically compare the subgroup to the subset of the overall group that does not include the subgroup.</p>

<p>Think about it this way.</p>

<p>If $p$ is the proportion that died, and $p-1$ is the proportion who did not die, and </p>

<p>$$\\bar{X}_. = p\\bar{X}_d + (p-1)\\bar{X}_a$$</p>

<p>where $\\bar{X}_.$ is the overall mean, $\\bar{X}_d$ is the mean of those that died, and $\\bar{X}_a$ is the mean of those that are still alive. Then if </p>

<p>$$\\bar{X}_d \\neq \\bar{X}_a$$
then </p>

<p>$$\\bar{X}_d \\neq \\bar{X}_.$$</p>

<p>Thus, researchers typically test the difference between the subgroup and the subset of the overall group that does not include the subgroup. This has the effect of showing that the subgroup differs from the overall group. It also allows you use conventional methods like an independent groups t-test.</p>
"
"21246","<p><strong>(1)</strong> Yes, that's correct. The null hypothesis is that the sub-model is sufficient and the extra parameters in the larger model are unnecessary. </p>

<p><strong>(2)</strong> Yes, as long as the null model is a sub-model of the alternative model, the likelihood ratio test is justified. </p>

<p><strong>(3)</strong> The theory of the likelihood ratio test is based on the exact value of the maximized log-likelihood, so this could be a concern. However, the approximate optimized log-likelihood could be an <strong>underestimate</strong> (but not an overestimate, since it cannot exceed the true maximum), so your observed test statistic will be too small, if anything, so the test result will be conservative (i.e. if you observe a significant result, you can be confident that it would still be significant if you had the true likelihood). </p>

<p><strong>(4)</strong> Yes. The likelihood ratio test statistic has a $\\chi^2$ distribution under the null hypothesis. </p>

<p><strong>(5)</strong> Use degrees of freedom equal to the number of parameters you had to delete to produce the submodel (1 in your case, apparently). You can use a table but unless you test statistic value is right on the table, you won't be able to calculate the exact $p$-value. You could use, for example, the pchisq function in R to calculate $p$-values. For example, </p>

<pre><code> 1-pchisq(LRT,df=D)
</code></pre>

<p>will give you the $p$-value when the observed test statistic is the variable LRT and the degrees of freedom is D. However, in your case your observed test statistic is $&gt;60000$ when you add only one parameter, resulting in a $p$-value of essentially 0. A jump of over 30000 log-likelihood points indicates either a very large effect and/or a very large sample size. So, you don't need a table to see that the additional parameter is very important. </p>
"
"52802","<p>If you tabulate within levels of p11, you will see several levels with all zero rows or very sparse entries:</p>

<pre><code> with( dados_censo, table(p13, p14, p11))
</code></pre>

<p>I suspect you are having problems in the estimation of the appropriate degrees of freedom and the construction of an adjustment to the Pearson-statistic. The data looks very sparse or with all-zero rows for p11 levels: 1-7, 9-12, 14-18, 20-28. Attempting to choose different adjustment methods fails. The failure is a solve call before the switching logic and all the methods use the same ""Delta"" matrix in one way or another</p>
"
"17797","<p>The answer can not be retrieved without the aid of computation.</p>

<p>If you look at the F distribution table for F(3,19), you'll see that (for F(3,20):</p>

<p>...</p>

<p>.050 | 3.10</p>

<p>.025 | 3.86</p>

<p>...</p>

<p>Which means 0.025 &lt; p &lt; 0.05.</p>

<p>I'm guessing that they ""cheated"" with MATLAB or something.</p>
"
"8053","<p>Suppose you have</p>

<p>Case 1:</p>

<blockquote>
  <p>A=200, B=100</p>
  
  <p>C=100, D=200</p>
</blockquote>

<p>versus</p>

<p>Case 2:</p>

<blockquote>
  <p>A=200, B=0</p>
  
  <p>C=200, D=200</p>
</blockquote>

<p>The B=0 in case 2 means that case 2 provides much stronger evidence than case 1 of a relationship between X and Outcome; but in your test, both cases would be scored the same.</p>

<p>The Chi-Square test, informally speaking, not only takes into account the ""X XOR Outcome"" relationship (which is what you test) but also ""X implies Outcome"", ""not X implies Outcome"" and so on.</p>
"
"27441","<p>It is the percentage change in the coefficient for LWD when AGE is added to the model: $$\\Delta\\beta\\%=(1.054-1.010)/1.054 \\times 100\\% = 4.2\\%$$.</p>

<p>1.054 is the coefficient for LWD in Model 1 in Table 3.14, and 1.010 is the coefficient for LWD in Model 2 (which has the potential confounder AGE included).</p>
"
"108379","<p>There are a variety of ways one might compare OLS and MLE fits.</p>

<p>One can clearly compute the likelihood or log-likelihood for a fit not obtained by ML (obviously ML does better -- because it maximizes the likelihood).</p>

<p>One can as readily compute the sums of squares of residuals for OLS and ML (obviously OLS wins -- because it minimizes this criterion).</p>

<p>One can compute some other measure of fit than either the likelihood or SSE.</p>

<p>An obvious class of distributions to consider would be the exponential family, and the use of GLMs. It includes the normal as a special case.</p>

<p>We can do regression for a gamma model and using OLS quite readily:</p>

<p><img src=""http://i.stack.imgur.com/YdArz.png"" alt=""enter image description here""></p>

<p>We might evaluate the likelihood for the OLS fit by substituting its parameter estimates into the likelihood function for the mean parameter in the gamma (if we use shape-mean parameterization). </p>
"
